# Copyright (c) Facebook, Inc. and its affiliates.
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Mostly copy-paste from timm library.
https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
"""
import copy
import math 
import os
import sys
import warnings

from transformers import ViTConfig
from safetensors.torch import load_file  
import timm 
from torch import Tensor, nn
from transformers import AutoModel, AutoConfig  
import torch
import torch.nn as nn

from transformers import AutoConfig, AutoModel, ViTConfig, Dinov2Config, Dinov2WithRegistersConfig
from torch.nn.utils.parametrizations import weight_norm 
from utils.config_utils import find_closest_model_config 
from utils.logger_utils import write_to_main_log
  

import torch
from torch import nn

from utils.model_utils import transfer_weights_from_pretrained
from utils.standardization import standardize_model_config_object 
class DINOiBOTWrapper(nn.Module): 
    def __init__(self, config, backbone, dino_head, ibot_head=None):
        super().__init__()
        self.config = config 
        self.backbone = backbone
        self.dino_head = dino_head
        self.ibot_head = ibot_head  
        self.n_global_crops = 2
 
    
    def forward(self, pixel_values, interpolate_pos_encoding=False ,mask_indices_list=None, shuffle_global=None): 
        if interpolate_pos_encoding:
            backbone_output = self.backbone(  pixel_values,interpolate_pos_encoding=interpolate_pos_encoding  )
        else: 
            backbone_output = self.backbone(  pixel_values ) 

        if hasattr(backbone_output, "last_hidden_state"):
            model_cls = backbone_output.last_hidden_state[:, 0]
            model_patches = backbone_output.last_hidden_state[:, 1:]
        else:
            model_cls = backbone_output[:, 0]
            model_patches = backbone_output[:, 1:]
 
        if shuffle_global:
            model_cls_chunks = model_cls.chunk(self.n_global_crops) 
            model_cls = torch.cat((model_cls_chunks[1], model_cls_chunks[0]))
        dino_output = self.dino_head(model_cls)
        model_masked_patches=None
        ibot_output=None
        if mask_indices_list is not None:
            model_masked_patches = torch.index_select(
                model_patches.flatten(0, 1),
                dim=0,
                index=mask_indices_list
            ) 
            if self.ibot_head: 
                ibot_output = self.ibot_head(model_masked_patches)
            else: 
                ibot_output = self.dino_head(model_masked_patches) 

        return dino_output, ibot_output
    
class DinoWrapper(nn.Module): 
    def __init__(self, config, backbone, dino_head, ibot_head=None):
        super().__init__()
        self.config = config 
        self.backbone = backbone
        self.dino_head = dino_head
        self.ibot_head = ibot_head  
 
    def forward(self, pixel_values, interpolate_pos_encoding=False ) : 
        if interpolate_pos_encoding:
            backbone_output = self.backbone(  pixel_values,interpolate_pos_encoding=interpolate_pos_encoding  )
        else: 

            backbone_output = self.backbone(  pixel_values ) 

        if hasattr(backbone_output, "last_hidden_state"):
            embeddings = backbone_output.last_hidden_state[:, 0]
        else:
            embeddings = backbone_output 
        output = self.dino_head(embeddings) 

        return output 
def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)
 


class DINOHead(nn.Module):
    def __init__(self, in_dim, out_dim, use_bn=False, norm_last_layer=True, nlayers=3, hidden_dim=2048, bottleneck_dim=256):
        super().__init__()
        nlayers = max(nlayers, 1)
        if nlayers == 1:
            self.mlp = nn.Linear(in_dim, bottleneck_dim)
        else:
            layers = [nn.Linear(in_dim, hidden_dim)]
            if use_bn:
                layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.GELU())
            for _ in range(nlayers - 2):
                layers.append(nn.Linear(hidden_dim, hidden_dim))
                if use_bn:
                    layers.append(nn.BatchNorm1d(hidden_dim))
                layers.append(nn.GELU())
            layers.append(nn.Linear(hidden_dim, bottleneck_dim))
            self.mlp = nn.Sequential(*layers)
        self.apply(self._init_weights)
        self.last_layer = weight_norm(nn.Linear(bottleneck_dim, out_dim, bias=False))
        
        self.last_layer.parametrizations.weight.original0.data.fill_(1)
        if norm_last_layer:
            self.last_layer.parametrizations.weight.original0.requires_grad = False

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        x = self.mlp(x)
        x = nn.functional.normalize(x, dim=-1, p=2)
        x = self.last_layer(x)
        return x

 
def build_transformer_model_from_config(model_type, accelerator, custom_config, load_pretrained, checkpoint_path=None,level_awareness=None):
    # 1. Load or create configuration
    try:
        model_config = AutoConfig.from_pretrained(model_type)
        for key, value in vars(custom_config).items():
            if key != "architectures":
                setattr(model_config, key, value)
        write_to_main_log(accelerator=accelerator, result=f"Loaded and customized configuration for {model_type}")
    except Exception as e:
        write_to_main_log(accelerator=accelerator, result=f"Error loading configuration: {e}", type='warning')
        
        # Create config based on model type
        config_dict = {**vars(custom_config), "_name_or_path": custom_config.model_type}
        config_class = {
            'vit': ViTConfig,
            'dinov2_with_registers': Dinov2WithRegistersConfig,
            'dinov2': Dinov2Config
        }.get(config_dict["model_type"])
        
        model_config = config_class(**config_dict) if config_class else None
        write_to_main_log(accelerator=accelerator, result=f"Created configuration directly from custom config")
    
    # 2. Initialize model
    model = AutoModel.from_config(model_config)
    write_to_main_log(accelerator=accelerator, result=f"Model initialized from configuration")
    
    # 3. Load weights (if needed)
    if checkpoint_path and not load_pretrained:
        # Load from checkpoint
        try:
            model = AutoModel.from_pretrained(checkpoint_path)
            write_to_main_log(accelerator=accelerator, result=f'Weights loaded from: {checkpoint_path}')
        except Exception as e:
            write_to_main_log(accelerator=accelerator, result=f'Failed to load weights from: {checkpoint_path}', type='error')
    
    elif load_pretrained:
        # Load pretrained weights
        if not _load_pretrained_weights(model, model_type, accelerator):
            # Fallback: transfer weights from closest compatible model
            _transfer_from_closest_model(model, custom_config, model_type, accelerator) 
    return model, model_config


def _load_pretrained_weights(model, model_type, accelerator):
    """Helper function to load pretrained weights."""
    try:
        checkpoint_path = os.path.join('checkpoints', model_type) 
        source_path = checkpoint_path if os.path.exists(checkpoint_path) else None
        print('source_path: ',source_path, model_type)
        if source_path: 
            pretrained_model = AutoModel.from_pretrained(source_path)

            write_to_main_log(accelerator=accelerator, result=f'Model weights loaded from local: {source_path}')
        else: 
            pretrained_model = AutoModel.from_pretrained(model_type)
            write_to_main_log(accelerator=accelerator, result=f'Model weights loaded from repo: {model_type}')
            
        missing, unexpected = model.load_state_dict(pretrained_model.state_dict(), strict=False)
        if missing:
            write_to_main_log(accelerator=accelerator, result=f"Missing keys ({len(missing)}): {missing}", type='warning')
        if unexpected:
            write_to_main_log(accelerator=accelerator, result=f"Unexpected keys ({len(unexpected)}): {unexpected}", type='warning')
            
        write_to_main_log(accelerator=accelerator, result=f'Pretrained weights loaded successfully')
        del pretrained_model
        return True
    except Exception as e:
        write_to_main_log(accelerator=accelerator, result=f"Error loading pretrained weights: {e}", type='error')
        return False


def _transfer_from_closest_model(model, custom_config, model_type, accelerator):
    """Helper function to transfer weights from closest compatible model."""
    write_to_main_log(accelerator=accelerator, result="Attempting weight transfer from closest model", type='info')
    
    closest_model_type = find_closest_model_config(custom_config, accelerator)
    if not closest_model_type:
        write_to_main_log(accelerator=accelerator, result="No suitable model found for weight transfer", type='warning')
        return
    
    try:
        # Try local checkpoint first, then remote
        checkpoint_path = os.path.join('checkpoints', closest_model_type)
        source_path = checkpoint_path if os.path.exists(checkpoint_path) else closest_model_type
        
        source_model = AutoModel.from_pretrained(source_path)
        write_to_main_log(accelerator=accelerator, result=f"Source model loaded from {source_path}")
        
        # Transfer weights
        model = transfer_weights_from_pretrained(model, source_model, method='mean', accelerator=accelerator)
        write_to_main_log(accelerator=accelerator, result=f"Successfully transferred weights from {closest_model_type}")
        
        del source_model
    except Exception as e:
        write_to_main_log(accelerator=accelerator, result=f"Error during weight transfer: {e}", type='error')

def build_dino_head_from_config(config,model_params):  
    in_dim =model_params.hidden_size
    out_dim=config.dino_head.out_dim
    use_bn_in_head=False
    norm_last_layer=config.dino_head.norm_last_layer 

    return DINOHead(  in_dim,  out_dim,  use_bn=use_bn_in_head,  norm_last_layer=norm_last_layer,) 

def build_ibot_head_from_config(config,model_params): 
    in_dim =model_params.hidden_size
    out_dim=config.ibot.out_dim
    use_bn_in_head=False
    norm_last_layer=config.dino_head.norm_last_layer 

    return DINOHead(  in_dim,  out_dim,  use_bn=use_bn_in_head,  norm_last_layer=norm_last_layer,) 

TIMM_LIST=['MahmoodLab/UNI2-h', 'MahmoodLab/UNI','prov-gigapath/prov-gigapath']
def build_transformer_model_from_timm(model_type, accelerator, load_from_disk=False, weights_path=None):
 
    
    write_to_main_log( accelerator= accelerator, result=  f"Building model for: {model_type}") 
    try:  
        if model_type in TIMM_LIST: 
            if model_type=='MahmoodLab/UNI': 
                return get_uni(load_from_disk=load_from_disk, weights_path= weights_path,accelerator=accelerator)
            if model_type=='MahmoodLab/UNI2-h': 
                return get_uni2(load_from_disk=load_from_disk, weights_path= weights_path,accelerator=accelerator)
            if model_type=='prov-gigapath/prov-gigapath': 
                return get_provgigapath(load_from_disk=load_from_disk, weights_path= weights_path,accelerator=accelerator)
        else: 
            raise NameError("Variable not defined in TIMM models.")
 
    except Exception as e:
        write_to_main_log( accelerator= accelerator, result= f"Error loading model: {e}", type='error')
         
        import torch.nn as nn
        
        model = nn.Module()   
        model_config = {}   
        write_to_main_log( accelerator= accelerator, result= "Returning empty fallback model", type='error')
        
        return model, model_config
    
    



def get_uni2(accelerator,  load_from_disk=False, weights_path=''): 
    down_path = "hf-hub:MahmoodLab/UNI2-h"
    base_arch_name = "vit_giant_patch14_224" # Specific architecture name for UNI2-h (example, verify if needed)
 
    explicit_timm_kwargs = {
                'img_size': 224, 
                'patch_size': 14, 
                'depth': 24,
                'num_heads': 24,
                'init_values': 1e-5, 
                'embed_dim': 1536,
                'mlp_ratio': 2.66667*2,
                'num_classes': 0, 
                'no_embed_class': True,
                'mlp_layer': timm.layers.SwiGLUPacked, 
                'act_layer': torch.nn.SiLU, 
                'reg_tokens': 8, 
                'dynamic_img_size': True
            }


    model = None

    if load_from_disk and weights_path: 
        try:
                # When loading from disk, use the base architecture name
            model = timm.create_model(
                    base_arch_name,
                    pretrained=False,
                    **explicit_timm_kwargs # Use explicit kwargs for structure
                )
            state_dict_path = os.path.join(weights_path, "pytorch_model.bin") # Verify this filename
            if os.path.exists(state_dict_path):
                model.load_state_dict(torch.load(state_dict_path, map_location="cpu", weights_only=True), strict=True)
                write_to_main_log(accelerator=accelerator, result= f"Loaded weights from local path: {state_dict_path}")
            else:
                write_to_main_log(accelerator=accelerator, result= f"Warning: Weights file not found at {state_dict_path}. Model initialized without local weights.", type='warning')
        except Exception as e:
            write_to_main_log(accelerator=accelerator, result= f"Error loading weights from disk: {e}", type='error')
            pass
        model.eval()
    else:
        # When loading pretrained, use the hub path
        write_to_main_log(accelerator=accelerator, result= f"Loading pretrained model from Hugging Face Hub: {down_path}")
        model = timm.create_model(
            down_path,
            pretrained=True , 
            **explicit_timm_kwargs
        )
        model.eval()
 
    model_config_dict = copy.deepcopy(explicit_timm_kwargs)

    # 1. Update with default_cfg if available (lower precedence)
    if hasattr(model, 'default_cfg') and model.default_cfg is not None:
        model_config_dict.update(dict(model.default_cfg))
        # print("Started config with model.default_cfg") # Optional: keep for debugging

    # 2. Update with pretrained_cfg if available (medium precedence)
    if hasattr(model, 'pretrained_cfg') and model.pretrained_cfg is not None:
         model_config_dict.update(dict(model.pretrained_cfg)) 
    # Add model_type (can be inferred or set explicitly)
    # Use the hub path or a specific name like 'UNI2-h'
    model_config_dict['model_type'] = down_path # Or 'UNI2-h' or base_arch_name

    # --- End Configuration Combination ---

    # Pass the combined dictionary to the standardization function
    standardized_config_object = standardize_model_config_object(model_config_dict)

    # Return the model instance and the standardized config object
    return model, standardized_config_object

def get_uni(accelerator, load_from_disk=False, weights_path=''): 
    down_path = "hf-hub:MahmoodLab/UNI"
    base_arch_name = "vit_large_patch16_224" # Specific architecture name

    # Define explicit kwargs for this architecture - these are the most reliable source
    explicit_timm_kwargs = {
        'img_size': 224,
        'patch_size': 16,
        'init_values': 1e-5, # Note: Your sample config has 1.0, this might be a difference
        'num_classes': 0,
        'dynamic_img_size': True,
        # Architectural parameters - define them here if known
        'embed_dim': 1024, # Example value for large model
        'depth': 24,       # Example value for large model
        'num_heads': 16,   # Example value for large model
        'mlp_ratio': 4,    # Example value
        # Add other specific kwargs if needed for this model variant
    }

    model = None

    if load_from_disk and weights_path:
        model = timm.create_model(
            base_arch_name, # Use the determined base architecture name
            pretrained=False,
            **explicit_timm_kwargs # Use explicit kwargs for structure
        )
        try:
            state_dict_path = os.path.join(weights_path, "pytorch_model.bin")
            if os.path.exists(state_dict_path):
                model.load_state_dict(torch.load(state_dict_path, map_location="cpu" ,weights_only=True), strict=True)
                write_to_main_log(accelerator=accelerator, result= f"Loaded weights from local path: {state_dict_path}")
            else:
                write_to_main_log(accelerator=accelerator, result= f"Warning: Weights file not found at {state_dict_path}. Model initialized without local weights.",type='warning')
        except Exception as e:
            write_to_main_log(accelerator=accelerator, result= f"Error loading weights from disk: {e}", type='error')
            pass
        model.eval()
    else: 
        model = timm.create_model(
            down_path,
            pretrained=True,
            **explicit_timm_kwargs # Use explicit kwargs for structure even with pretrained
        )
        model.eval()
 
    model_config_dict = copy.deepcopy(explicit_timm_kwargs)

    # 1. Update with default_cfg if available (lower precedence)
    if hasattr(model, 'default_cfg') and model.default_cfg is not None:
        # Update the dictionary, existing keys from explicit_timm_kwargs will be kept
        # New keys from default_cfg will be added
        model_config_dict.update(dict(model.default_cfg))
        # print("Started config with model.default_cfg") # Optional: keep for debugging

    # 2. Update with pretrained_cfg if available (medium precedence)
    if hasattr(model, 'pretrained_cfg') and model.pretrained_cfg is not None:
         # Update again, keys from pretrained_cfg will override default_cfg but not explicit_timm_kwargs
         model_config_dict.update(dict(model.pretrained_cfg))
         # print("Updated config with model.pretrained_cfg") # Optional: keep for debugging
 
    model_config_dict['model_type'] = base_arch_name 
    # --- End Configuration Combination ---

    # Pass the combined dictionary to the standardization function
    standardized_config_object = standardize_model_config_object(model_config_dict)

    # Return the model instance and the standardized config object
    return model, standardized_config_object


def get_provgigapath(accelerator,load_from_disk=False, weights_path=''):
    
    down_path = "hf_hub:prov-gigapath/prov-gigapath"
    # The architecture name used in timm.create_model for this model
    base_arch_name = "vit_giant_patch14_dinov2" # Based on your provided config structure
 
    explicit_timm_kwargs = {
        # Top-level keys from your sample config
        "img_size": 224,
        "in_chans": 3,
        "patch_size": 16, # Note: Sample config has 16 here, but vit_giant_patch14 implies 14. Verify.
        "embed_dim": 1536, # This is num_features/hidden_size
        "depth": 40,
        "num_heads": 24,
        "init_values": 1e-05,
        "mlp_ratio": 5.33334,
        # num_classes is duplicated, already at top level
    }

    model = None

    if load_from_disk and weights_path:
        # When loading from disk, use the base architecture name
        try: 
            model = timm.create_model(
                base_arch_name,
                pretrained=False,
                **explicit_timm_kwargs # Use explicit kwargs for structure
            )
            state_dict_path = os.path.join(weights_path, "pytorch_model.bin") # Example filename
            
            if os.path.exists(state_dict_path):
                model.load_state_dict(torch.load(state_dict_path, map_location="cpu",weights_only=True), strict=True)
                write_to_main_log(accelerator=accelerator, result= f"Loaded weights from local path: {state_dict_path}")
            else:
                write_to_main_log(accelerator=accelerator, result= f"Warning: Weights file not found at {state_dict_path}. Model initialized without local weights.",type='warning')
        except Exception as e:
            write_to_main_log(accelerator=accelerator, result= f"Error loading weights from disk: {e}", type='error')
            pass
        model.eval()
    else:
        # When loading pretrained, use the hub path
        write_to_main_log(accelerator=accelerator, result= f"Loading pretrained model from Hugging Face Hub: {down_path}")
        model = timm.create_model(
            down_path,
            pretrained=True
        )
        model.eval()
 
    model_config_dict = copy.deepcopy(explicit_timm_kwargs)

    # 1. Update with default_cfg if available (lower precedence)
    if hasattr(model, 'default_cfg') and model.default_cfg is not None:
        model_config_dict.update(dict(model.default_cfg))
        # print("Started config with model.default_cfg") # Optional: keep for debugging

    # 2. Update with pretrained_cfg if available (medium precedence)
    if hasattr(model, 'pretrained_cfg') and model.pretrained_cfg is not None:
         model_config_dict.update(dict(model.pretrained_cfg)) 
    model_config_dict['model_type'] = down_path # Or 'prov-gigapath' or base_arch_name

    # --- End Configuration Combination ---

    # Pass the combined dictionary to the standardization function
    standardized_config_object = standardize_model_config_object(model_config_dict)

    # Return the model instance and the standardized config object
    return model, standardized_config_object